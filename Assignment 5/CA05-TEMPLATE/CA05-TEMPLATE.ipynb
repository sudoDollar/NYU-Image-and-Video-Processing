{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment 5\n",
    "## Feature Detection, Image Stitching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import convolve as convolveim\n",
    "import scipy.ndimage.filters as filters\n",
    "import scipy.ndimage as ndimage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART A - Harris detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write your own program for Harris corner point detection at a fixed scale. Your program should contain the following steps:\n",
    " - (a)  Generate gradient images of Ix and Iy using filters corresponding to derivative of Gaussian functions of a chosen **scale σ and window size w** (let us use w=4σ+1). You can use the convolve function from scipy.\n",
    " - (b) Compute three images **Ix^2 , Iy^2 , Ix* Iy**.\n",
    " - (c) To determine the Harris value at each pixel, we should apply Gaussian weighting over a window size of WxW centered at this pixel to each of the image Ix^2, Iy^2, and Ix Iy, and then sum the weighted average. This is equivalent to convolve each of these images by a Gaussian filter with size WxW. Let us use a Gaussian filter with scale 2σ, and window size W=6σ+1.\n",
    " - (d) Generate an image of Harris corner value based on the images from (c). See slide 18 and 19 from _feature detection_ lecture notes.\n",
    " - (e) Detect local maxima in the Harris value image (For each pixel, look at a 7x7 window centered at the pixel. Keep the pixel only if it is greater than all other pixels in this window). **Pick the first N feature points with largest Harris values.**\n",
    " - (f) Mark each detected point using a small circle. Display the image with the detected points.\n",
    " - You should write your own functions to generate Guassian and derivative of Gaussian filters from the analytical forms (This is similar to Part2 in CA02).\n",
    "- Apply your Harris detector to a test image (you can just work on gray scale image). Using σ=1, N=100. Do the features detected make sense?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(size, sigma):\n",
    "    \"\"\"\n",
    "    This function will generate 3 filters given the size of the filter and sigma of Gaussian: \n",
    "    1: gaussian filter;\n",
    "    2: derivative of gaussian filters in x and y direction.\n",
    "    \"\"\"\n",
    "    # define the x range\n",
    "    x_ax = np.arange(0,size) - size/2 + 0.5\n",
    "        \n",
    "    ################################################ TODO ###############################################\n",
    "    # make 1D gaussian filter\n",
    "    gauss = ... \n",
    "    # Compose 2D Gaussian filter from 1D, using the separability property of 2D Gaussian\n",
    "    gauss2 = ... \n",
    "    # Normalize the filter so that all coefficients sum to 1\n",
    "    gauss1 = ... \n",
    "    \n",
    "    # Create derivatives of gaussian \n",
    "    gauss1_dx = np.matrix(np.zeros((np.shape(gauss1)), dtype=\"float32\"))\n",
    "    gauss1_dy = np.matrix(np.zeros((np.shape(gauss1)), dtype=\"float32\"))\n",
    "    for j in range(0, len(x_ax)):\n",
    "        # derivative filter in x\n",
    "        gauss1_dx[:, j] = (gauss1[:, j] * (-x_ax[j])/(sigma*sigma)).reshape(size,1) \n",
    "        ################################################ TODO ###############################################\n",
    "        # similarly define the difference in y\n",
    "        gauss1_dy[j, :] = ...  \n",
    "    return gauss1,gauss1_dx, gauss1_dy\n",
    "\n",
    "# Visualize the filters you created to make sure you are working with the correct filters\n",
    "gauss1, gauss1_dx, gauss1_dy = gauss(5,1)\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(gauss1)\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(gauss1_dx)\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(gauss1_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harris(Ix, Iy , input_image,N):\n",
    "    \"\"\"\n",
    "    The input to this function are the gradient images in x and y directions, the original image and N\n",
    "    The function will output two arrays/lists x and y which are the N points with largest harris values,\n",
    "    and an image of the harris values.\n",
    "    \"\"\"\n",
    "    l, m = np.shape(input_image)\n",
    "    ################################################ TODO ###############################################\n",
    "    #Forming 3 images\n",
    "    #Ix square\n",
    "    Ix2 = ... \n",
    "    #Iy square\n",
    "    Iy2 = ... \n",
    "    #Ix*Iy\n",
    "    Ixy = ... \n",
    "    \n",
    "    # Smooth image Ix2, Iy2, Ixy  with Gaussian filter with sigma=2, size=7.\n",
    "    # Get the gauss filter for smoothing (reuse what you have)\n",
    "    gauss_smooth, _, _ = ...\n",
    "    Ix2_smooth = convolveim(Ix2, gauss_smooth, mode='nearest')\n",
    "    ################################################ TODO ###############################################\n",
    "    # CONVOLVE as shown above\n",
    "    Iy2_smooth = ...  \n",
    "    Ixy_smooth = ...\n",
    "    # By doing this, Ix2_smooth, Iy2_smooth, Ixy_smooth are the three values needed to calculate \n",
    "    # the A matrix for each pixel.\n",
    "    \n",
    "    ################################################ TODO ###############################################\n",
    "    # Write code segment to find N harris points in the image\n",
    "    # Refer to the page 17 of slides on features for the equation\n",
    "    det = ...\n",
    "    trace = ...\n",
    "    H = ...\n",
    "    \n",
    "    ################################################ TODO ###############################################\n",
    "    # Save a copy of the original harris values before detecting local max\n",
    "    H0 = H    \n",
    "    # Detect local maximum over 7x7 windows\n",
    "    local_max_win = 7\n",
    "    a = int(np.floor(local_max_win/2))\n",
    "    H = np.pad(H,((a,a),(a,a)), 'constant')\n",
    "    # Initialize a mask to be all ones. The mask corresponds to the local maximum in H\n",
    "    H_max = np.ones(H.shape)      \n",
    "    for i in range(a,l+a):\n",
    "        for j in range(a,m+a):\n",
    "            # Take a WxW patch centered at point (i,j), check if the center point is larger than all other points\n",
    "            # in this patch. If it is NOT local max, set H_max[i,j] = 0\n",
    "            patch = ...\n",
    "            ...\n",
    "    \n",
    "    # Multiply the mask with H, points that are not local max will become zero\n",
    "    H = H_max*H      \n",
    "    H = H[a:-a,a:-a]\n",
    "    \n",
    "    # Find largest N points' coordinates\n",
    "    # Hint: use np.argsort() and np.unravel_index() to sort H and get the index in sorted order\n",
    "    ...\n",
    "    \n",
    "    # x,y should be arrays/lists of x and y coordinates of the harris points.\n",
    "    return x,y,H0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IMPORTANT: Convert your image to float once you load the image. ######\n",
    "input_image = cv2.imread('9.png',0).astype('float')\n",
    "\n",
    "img = cv2.normalize(input_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# Generating the gaussian filter\n",
    "sigma = 1\n",
    "size = int(4*sigma + 1)\n",
    "# Function call to gauss\n",
    "gauss_filt, gauss_filt_dx, gauss_filt_dy = ... \n",
    "\n",
    "    \n",
    "################################################ TODO ###############################################\n",
    "# Convolving the filter with the image\n",
    "# Convolve image with dx filter\n",
    "Ix = convolveim(input_image,gauss_filt_dx,mode ='nearest') \n",
    "# Convolve image with dy filter\n",
    "Iy = ... \n",
    "\n",
    "x,y,H0 = harris(Ix, Iy ,input_image,100)\n",
    "################################################ TODO ###############################################\n",
    "# Plot: Ix, Iy, Original image with harris point labeled in red dots, H0 harris value image\n",
    "# Hint: you may use \"plt.plot(y,x, 'ro')\"   # Note: x is vertical and y is horizontal in our above definition\n",
    "                                            # But when plotting the point, the definition is reversed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART B - SIFT descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program that can generate SIFT descriptor for each detected feature point using your program in Prob. 1. You may follow the following steps:\n",
    "- Generate gradient images Ix and Iy as before. Furthermore, determine the gradient magnitude and orientation from Ix and Iy at every pixel.\n",
    "-  Quantize the orientation of each pixel to one of the N=8 bins. Suppose your original orientation is x. To quantize the entire range of 360 degree to 8 bins, the bin size is q=360/N=45 degree. Assuming your orientation is determined with a range of [0,360]. x_q will range from 0 to 7, with 0 corresponding to degree (0, 22.5) and (360-22.5, 360). You can perform quantization using:  x_q=floor((x+q/2)/q); but if x_q=N, change to -> x_q=0\n",
    "- Then for each detected feature point, follow these steps to generate the SIFT descriptor:\n",
    "    - i) Generate a patch of size 16x16 centered at the detected feature point;\n",
    "    - ii) Multiply the gradient magnitude with a Guassian window with scale= patch width/2. \n",
    "    - iii) Generate a HoG for the entire patch using the weighted gradient magnitude.\n",
    "    - iv) Determine the dominant orientation of the patch by detecting the peak in the Hog determined in (iii).\n",
    "    - v) Generate a HoG for each of the 4x4 cell in the 16x16 patch.\n",
    "    - vi) Shift each HoG so that the dominant orientation becomes the first bin. \n",
    "    - vii) Concatenate the HoG for all 16 cells into a single vector.\n",
    "    - viii) Normalize the vector. That is, divide each entry by L2 norm of the vector. \n",
    "    - ix) Clip the normalized vector so that entries >0.2 is set to 0.2.\n",
    "    - x) Renormalize the vector resulting from (ix).\n",
    "-  **For this assignment, you are not asked to do multiscale processing. You only need to generate the SIFT descriptors for those feature points detected by the Harris detector at the original image scale (from Part A).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histo(theta4,mag4):\n",
    "    \"\"\"\n",
    "    theta4: an array of quantized orientations, with values 0,1,2...7.\n",
    "    mag4: an array of the same size with magnitudes\n",
    "    \"\"\"\n",
    "    temp = np.zeros((1,8),dtype='float32')\n",
    "    ################################################ TODO ###############################################\n",
    "    # write code segment to add the magnitudes of all vectors in same orientations\n",
    "    ...\n",
    "    \n",
    "    # temp should be a 1x8 vector, where each value corresponds to an orientation and \n",
    "    # contains the sum of all gradient magnitude, oriented in that orientation\n",
    "    return temp\n",
    "\n",
    "def descriptor(theta16,mag16):\n",
    "    \"\"\"\n",
    "    Given a 16x16 patch of theta and magnitude, generate a (1x128) descriptor\n",
    "    \"\"\"\n",
    "    filt,_,_ = gauss(16,8) \n",
    "    mag16_filt = mag16*filt\n",
    "\n",
    "    # array to store the descriptor. Note that in the end descriptor should have size (1, 128)\n",
    "    desp = np.array([])\n",
    "    ################################################ TODO ###############################################\n",
    "    # Make function call to histo, with arguments theta16 and mag16_fil\n",
    "    # This is used for find the location of maximum theta\n",
    "    histo16 = ... \n",
    "    maxloc_theta16 = ...\n",
    "\n",
    "    for i in range(0,16,4):\n",
    "        for j in range(0,16,4):\n",
    "            ################################################ TODO ###############################################\n",
    "            # Use histo function to create histogram of oriantations on 4x4 pathces in the neighbourhood of the harris points\n",
    "            # You should shift your histogram for each cell so that the dominant orientation of the 16x16 patch becomes the first quantized orientation\n",
    "            # You should update the variable desp to store all the orientation magnitude sum for each sub region of size 4x4\n",
    "            ...\n",
    "\n",
    "    ################################################ TODO ###############################################\n",
    "    # normalize descriptor, clip descriptor, normalize descriptor again\n",
    "    desp = ...\n",
    "    \n",
    "    \n",
    "    desp = np.matrix(desp)\n",
    "\n",
    "    return desp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_B(input_image):\n",
    "\n",
    "    # Normalize the image\n",
    "    img = cv2.normalize(input_image, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    # Generate derivative of Gaussian filters, using sigma=1, filter window size=4*sigma+1\n",
    "    sigma = 1\n",
    "    _, filt_dx, filt_dy = ...\n",
    "    \n",
    "    ################################################ TODO ###############################################\n",
    "    # Image convolved with filt_dx and filt_dy\n",
    "    img_x = ...    \n",
    "    img_y = ... \n",
    "\n",
    "    # Calculate magnitude and theta, then quantize theta.\n",
    "    mag = np.sqrt(img_x ** 2 + img_y ** 2)\n",
    "    theta = np.arctan2(img_y, img_x)\n",
    "    theta = (theta/(2*np.pi))*360\n",
    "    theta = theta*(theta>=0) + (360+theta)*(theta < 0)\n",
    "    \n",
    "    ################################################ TODO ###############################################\n",
    "    # Quantize theta to 0,1,2,... 7, see instructions above\n",
    "    q = 45\n",
    "    N = 8\n",
    "    theta_q = \n",
    "    \n",
    "    ################################################ TODO ###############################################\n",
    "    # Call harris function to find 100 feature points \n",
    "    x,y,_ = ... \n",
    "    \n",
    "    # Pad 15 rows and columns. You will need this extra border to get a patch centered at the feature point \n",
    "    #    when the feature points lie on the original border of the image.\n",
    "    theta_q = cv2.copyMakeBorder(theta_q.astype('uint8'), 7,8,7,8, cv2.BORDER_REFLECT)\n",
    "    ################################################ TODO ###############################################\n",
    "    mag =  ... # similarly add border to the magnitude image\n",
    "    final_descriptor = np.zeros((1,128))\n",
    "\n",
    "    for i in range(final_points.shape[0]):\n",
    "        # Since you have already added 15 rows and columns, now the new coordinates of the feature points are (x+8, y+8).\n",
    "        # Then the patch should be [x[i]:x[i]+16,y[i]:y[i]16]\n",
    "        # Your patch should be centered at the feature point.\n",
    "        theta_temp = theta_q[x[i]:x[i]+16,y[i]:y[i]+16] \n",
    "        # similarly, take a 16x16 patch of mag around the point\n",
    "        mag_temp = ... \n",
    "        # function call to descriptors\n",
    "        temp2 = ... \n",
    "        final_descriptor = np.vstack((final_descriptor,temp2))\n",
    "\n",
    "    # Initially, final descriptor has a row of zeros. We are deleting that extra row here.\n",
    "    final_descriptor = np.delete(final_descriptor,0,0)\n",
    "    final_descriptor = np.nan_to_num(final_descriptor)\n",
    "    final_descriptor = np.array(final_descriptor)\n",
    "    \n",
    "    # Combine x,y to form an array of size (Npoints,2) each row correspond to (x,y)\n",
    "    # You could use np.hstack() or np.vstack()\n",
    "    final_points = ...\n",
    "    \n",
    "    return final_descriptor,final_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_image = cv2.imread('9.png',0).astype('float') # input image\n",
    "\n",
    "\n",
    "# Visualization the results. Plot the feature point similiar to Part1 and plot SIFT features as bar\n",
    "final_descriptor , final_points = part_B(input_image)\n",
    "\n",
    "for i in range(0,10):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ax1.imshow(input_image,cmap='gray')\n",
    "    ax1.autoscale(False)\n",
    "    ax1.plot(final_points[i][1],final_points[i][0], 'ro')\n",
    "    ax2.bar(np.arange(1,129),final_descriptor[i,:])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART C - correspondance in 2 images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding corresponding points in two images based on SIFT descriptors.\n",
    "-  Using your program in Part A and Part B to detect feature points and generate their descriptors for two images provided with this assignment.(image named **left** and **right**)\n",
    "-  Write a program that can find matching points between the two images. For each detected point p in the first image, compute its distance to each detected point in the second image (using Euclidean distance between two SIFT descriptors, not spatial distance) to find two closest points q1 and q2 in the second image. Let us call the distance of p to q1 and q2 by d1 and d2, you will take point q1 as the matching point for p if d1/d2 <r. Otherwise, you assume there is ambiguity between q1 and q2 and skip the feature point p in image 1. You can experiment with different threshold r (for example, r=[0.95, 0.8, 0.65, 0.5]). Obviously r should be <1. At the end of this process, you should have a set of matching pairs.\n",
    "-  Create an image that shows the matching results. For example you can create a large image that has the left and right images side by side, and draw lines between matching pairs in these two images. Do the matched points look reasonable? You can use **cv2.line()** to draw line between each matching pair. Display the image after you add lines into the image array using the cv2.line() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('left.jpg',0).astype('float') # read left image\n",
    "img_1 = cv2.normalize(img1, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "descriptor_1, keypoints_1 = part_B(img_1)\n",
    "\n",
    "img2 = cv2.imread('right.jpg',0).astype('float') # read right image\n",
    "img_2 = cv2.normalize(img2, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "descriptor_2, keypoints_2 = part_B(img_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detected points in the two images\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(img_1,'gray')\n",
    "plt.plot(keypoints_1[:,1],keypoints_1[:,0],'ro',ms=3)\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.imshow(img_2,'gray')\n",
    "plt.plot(keypoints_2[:,1],keypoints_2[:,0],'ro',ms=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function to find corresponding points in image\n",
    "def points_matching(kp1, descriptor1, kp2, descriptor2, threshold):\n",
    "    matched_loca = list() # list of all corresponding points pairs. Point pairs can be stored as tuples\n",
    "    ################################################ TODO ###############################################\n",
    "    # Find matching points between img1 and img2 using the algorithm described in the above\n",
    "    # For distance measuring, you may use np.linalg.norm()\n",
    "    # You could implement it as nested loop for simplicity. \n",
    "    \n",
    "    return matched_loca\n",
    "\n",
    "\n",
    "# Test different thresholds for the matching\n",
    "for r in [0.95, 0.8, 0.65, 0.5]: \n",
    "    matched_loca = points_matching(keypoints_1, descriptor_1, keypoints_2, descriptor_2, r)\n",
    "    final_image = np.concatenate((img_1,img_2),axis=1)\n",
    "    print('threshold: ', r)\n",
    "    print('number of corresponding poitnts found:',len(matched_loca))\n",
    "    ################################################ TODO ###############################################\n",
    "    # Write code segment to draw lines joining corresponding points\n",
    "    # Use cv2.line() to draw the line on final_image\n",
    "    # Remember the x,y coordinate in numpy and OpenCV is opposite and you need to add image width for pt2\n",
    "    for pt in matched_loca:\n",
    "        ...\n",
    "        \n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(final_image,cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART D - panorama stiching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stitch two images into a panorama using SIFT feature detector and descriptor.\n",
    "In this part, you may use functions from cv2.\n",
    "- Read in the following image pair (left and right)\n",
    "- Detect SIFT points and extract SIFT features from each image by using the following OpenCV sample code. \n",
    "\n",
    "\n",
    "\n",
    "    - sift = cv2.SIFT_create()\n",
    "    - skp = sift.detect(img,None)\n",
    "    - (skp, features) = sift.compute(img, skp)\n",
    "\n",
    "\n",
    "\n",
    "[comment]: ![](q_img.png)\n",
    " - **Where skp is a list of all the key points found from img and features is the descriptor for the image. Each element in skp is an OpenCV ‘key points class’ object, and you can check the corresponding coordinate by skp[element_index].pt**\n",
    "- Detect and mark feature points, calculate their descriptor using cv2 functions.\n",
    "- Find the corresponding point pairs between left and right images based on their SIFT descriptors. You can reuse your program for Part-C.\n",
    "- Apply RANSAC method to these matching pairs to find the largest subset of matching pairs that are related by the same homography. You can use the function cv2.findHomography(srcPoints, dstPoints, cv2.RANSAC)\n",
    "- Create an image that shows the matching results by drawing lines between corresponding points. You can use the drawMatches function below\n",
    "- Apply the homography to the right image. You can use cv2.warpPerspective() to apply the homography transformation to the image.\n",
    "- Stitch the transformed right image and the left image together to generate the panorama.\n",
    "\n",
    "\n",
    "**In your report, show the left and right images, the left and right images with SIFT points indicated, the image that illustrates the matching line between corresponding points, the transformed left image, and finally the stitched image.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawMatches(imageA, imageB, kpsA, kpsB, matches, status, lw=1):\n",
    "    # initialize the output visualization image\n",
    "    (hA, wA) = imageA.shape[:2]\n",
    "    (hB, wB) = imageB.shape[:2]\n",
    "    vis = np.zeros((max(hA, hB), wA + wB), dtype=\"uint8\")\n",
    "    vis[0:hA, 0:wA] = imageA\n",
    "    vis[0:hB, wA:] = imageB\n",
    "\n",
    "    # loop over the matches\n",
    "    for ((trainIdx, queryIdx), s) in zip(matches, status):\n",
    "        # only process the match if the keypoint was successfully\n",
    "        # matched\n",
    "        if s == 1:\n",
    "            # draw the match\n",
    "            ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1]))\n",
    "            ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1]))\n",
    "            cv2.line(vis, ptA, ptB, (255, 255, 255), lw)\n",
    "    # return the visualization\n",
    "    return vis\n",
    "\n",
    "img1 = cv2.imread('left.jpg',0) # read left image\n",
    "img2 = cv2.imread('right.jpg',0) # read right iamge\n",
    "\n",
    "# Depending on your OpenCV version, you could set up SIFT differently\n",
    "sift = cv2.SIFT_create()\n",
    "# sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# Use sift.detect to detect features in the images\n",
    "kp1 = ...\n",
    "kp2 = ...\n",
    "\n",
    "# Visualize the keypoints\n",
    "img1_kps = cv2.drawKeypoints(img1,kp1,None,flags = cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_kps = ...\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img1_kps)\n",
    "plt.subplot(122)\n",
    "plt.imshow(img2_kps)\n",
    "plt.title('images with keypoints')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ TODO ###############################################\n",
    "# Use sift.compute to generate sift descriptors/features\n",
    "(kp1, features1) = sift.compute(img1,kp1)\n",
    "(kp2, features2) = ...\n",
    "\n",
    "\n",
    "kp1 = np.float32([kp.pt for kp in kp1])\n",
    "kp2 = np.float32([kp.pt for kp in kp2])\n",
    "\n",
    "matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "################################################ TODO ###############################################\n",
    "# Use knnMatch function in matcher to find corresonding features\n",
    "# For robustness of the matching results, we'd like to find 2 best matches (i.e. k=2 for knnMatch) \n",
    "# and return their matching distances \n",
    "rawMatches = matcher.knnMatch(...)\n",
    "matches = []\n",
    "\n",
    "# Now we validate if the matching is reliable by checking if the best maching distance is less than \n",
    "# the second matching by a threshold, for example, 20% of the 2nd best maching distance\n",
    "for m in rawMatches:\n",
    "    ################################################ TODO ###############################################\n",
    "    # Ensure the distance is within a certain ratio of each other (i.e. Lowe's ratio test)\n",
    "    # Test the distance between points. use m[0].distance and m[1].distance\n",
    "    if len(m) == 2 and ... : \n",
    "        matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "\n",
    "ptsA = np.float32([kp1[i] for (_,i) in matches])\n",
    "ptsB = np.float32([kp2[i] for (i,_) in matches])\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "### Similar to what we did in part C\n",
    "### Create an image img_match that shows the matching results by drawing lines between corresponding points. \n",
    "img_match = ...\n",
    "for p1, p2 in zip(ptsA, ptsB):\n",
    "    cv2.line(...)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img_match,'gray')\n",
    "plt.title('Matching points (Before RANSAC)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ TODO ###############################################\n",
    "# Find homography with RANSAC\n",
    "(H, status) = cv2.findHomography(...)\n",
    "\n",
    "img_ransac = drawMatches(img1,img2,kp1,kp2,matches,status)\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img_ransac,'gray')\n",
    "plt.title('Matching points (After RANSAC)')\n",
    "plt.show()\n",
    "\n",
    "################################################ TODO ###############################################\n",
    "# fill in the arguments to warp the second image to fit the first image.\n",
    "# For the size of the resulting image, you can set the height to be the same as the original, width to be twice the original.\n",
    "# First transform the right image, then fill in the left part with the orignal left image\n",
    "result = cv2.warpPerspective(...)\n",
    "# For blending, you could just overlay img1 to the corresponding positions on warped img2\n",
    "result[...] = img1\n",
    "\n",
    "plt.figure(figsize=(20,40))\n",
    "plt.imshow(result,'gray')\n",
    "plt.title('Stitched image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please answer the following questions based on your observation:\n",
    "* For these 2 images, the matched features points are not necessary from the same depth (and therefore not on the same plane), why we could still relate them by a homography?\n",
    "* Why the right image looks a bit blurry?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Correspondence Visualization\n",
    "* Just for interactive visualization, not in assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_idx = status.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_match(x):\n",
    "    \"\"\"\n",
    "    This function visualize the matches interactively\n",
    "    You could change the visualization of the matching keypoints by toggling a bar\n",
    "    Need to have the matches and status ready\n",
    "        matches: coarse matching results obtained from knnMatch\n",
    "        status: the refined matching results provided by cv2.findHomography,\n",
    "                the positive match determined by RANSAC is marked with 1,\n",
    "                while the negative match is marked with 0\n",
    "    \"\"\"\n",
    "    idx = matched_idx[x]\n",
    "    img_ransac = drawMatches(img1,img2,kp1,kp2,matches[idx:idx+1],status[idx:idx+1], lw=2)\n",
    "\n",
    "    plt.figure(figsize=(25,25))\n",
    "    plt.imshow(img_ransac,'gray')\n",
    "    plt.title('Matching points (After RANSAC)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(visualize_match, x=widgets.IntSlider(min=0, max=len(matched_idx)-1, step=1, value=100));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
